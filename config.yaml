# Project Configuration with Hardware Overrides
experiment_name: "frankenstein-full-curriculum"
data_dir: "data"
output_dir: "outputs"
device: "cuda"
mixed_precision: true
gradient_checkpointing: true
seed: 42

# Model Architecture (125M parameters)
model:
  d_model: 768
  n_layers: 12
  n_heads: 12
  d_ff: 2048
  vocab_size: 25000
  max_seq_len: 512
  dropout: 0.1
  attention_dropout: 0.1
  activation: "swiglu"
  norm_type: "rmsnorm"
  norm_eps: 0.000001
  pos_encoding: "rope"
  init_std: 0.02
  use_bias: false

# Training Configuration - NOW WITH HARDWARE OVERRIDES
training:
  # Optimization
  learning_rate: 0.0002
  min_learning_rate: 0.00002
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  eps: 0.00000001
  grad_clip_norm: 1.0
  warmup_steps: 1000
  scheduler: "cosine_with_restarts"
  
  # Batch settings (adjust for your GPU)
  batch_size: 16                    # Reduced from 32 to fit in memory
  gradient_accumulation_steps: 2    # Maintain effective batch size of 32
  max_grad_norm: 1.0
  
  # Training length
  max_epochs: 300
  eval_every: 1000
  save_every: 5000
  log_every: 100
  patience: 10
  min_delta: 0.0001
  val_split: 0.1
  
  # Memory optimizations
  use_gradient_checkpointing: true
  use_mixed_precision: true
  dataloader_num_workers: 2         # Reduced for stability
  pin_memory: false                 # Disabled to save memory
  label_smoothing: 0.0
  dropout_schedule: "constant"
  
  # HARDWARE CONFIGURATION - OVERRIDE HARDCODED VALUES
  max_vram_gb: 8.0                  # Your RTX 3070 Ti limit
  memory_safety_margin: 0.15       # 15% safety margin (more conservative)
  bytes_per_param: 4                # 4 bytes per parameter (FP32)
  memory_multiplier: 3.0            # 3x for model + optimizer + gradients

# Generation Configuration  
generation:
  temperature: 0.8
  top_k: null
  top_p: null
  num_steps: 50
  confidence_schedule: "exponential"
  max_length: 512
  num_return_sequences: 1
  vocab_level: 5
  auto_detect_vocab_level: true
  style_strength: 1.0
  interactive_mode: false
  skip_special_tokens: true
  clean_up_tokenization_spaces: true

# Curriculum Learning
curriculum:
  stages:
    - name: "foundation"
      epochs: 50
      masking_rate_range: [0.75, 0.90]
      data_selection: "easy"
      format_type: "sentences"
    - name: "structural"  
      epochs: 100
      masking_rate_range: [0.40, 0.60]
      data_selection: "medium"
      format_type: "pairs"
    - name: "refinement"
      epochs: 150
      masking_rate_range: [0.10, 0.30]
      data_selection: "all"
      format_type: "paragraphs"
  
  lexical_weight: 0.3
  syntactic_weight: 0.3
  centrality_weight: 0.4
  easy_threshold: 33.0
  hard_threshold: 67.0
  n_clusters: 8
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  transition_epochs: 5
  reset_optimizer: true
  pseudo_data_max_samples: 100
  pseudo_data_ratio: 0.25